{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd74bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stent\\OneDrive\\Desktop\\DeepLearning-assignments\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import wget, os, gzip, pickle, random, re, sys, importlib, tqdm, math, os, gzip, re, string\n",
    "\n",
    "from tqdm import trange\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "\n",
    "import random\n",
    "from random import choice\n",
    "\n",
    "IMDB_URL = 'http://dlvu.github.io/data/imdb.{}.pkl.gz'\n",
    "IMDB_FILE = 'imdb.{}.pkl.gz'\n",
    "WP_DATA = 'https://codeberg.org/pbm/former/raw/branch/master/data/enwik8.gz'\n",
    "\n",
    "PAD, START, END, UNK = '.pad', '.start', '.end', '.unk'\n",
    "\n",
    "SENT = '_s'\n",
    "TOY = {\n",
    "    '_s': ['_s _adv', '_np _vp', '_np _vp _prep _np', '_np _vp ( _prep _np )', '_np _vp _con _s','_np _vp ( _con _s )'],\n",
    "    '_adv': ['briefly', 'quickly', 'impatiently'],\n",
    "    '_np': ['a _noun', 'the _noun', 'a _adj _noun', 'the _adj _noun'],\n",
    "    '_prep': ['on', 'with', 'to', 'for', 'at'],\n",
    "    '_con': ['while', 'but'],\n",
    "    '_noun': ['mouse', 'bunny', 'cat', 'dog', 'man', 'woman', 'person', 'bear', 'koala', 'judge', 'businessman',\n",
    "        'businesswoman', 'lawyer', 'teacher', 'engineer'],\n",
    "    '_vp': ['walked', 'walks', 'ran', 'runs', 'goes', 'went', 'hiked'],\n",
    "    '_adj': ['short', 'quick', 'busy', 'nice', 'gorgeous', 'spectacular', 'reluctant', 'systematic', 'willowy', 'engaged', 'synthetic']\n",
    "}\n",
    "\n",
    "PRINTABLE = set(ord(c) for c in (string.digits + string.ascii_letters + string.punctuation + string.whitespace))\n",
    "\n",
    "def cas(i):\n",
    "    \"\"\"\n",
    "    Character-as-string. Filters out the ascii codes that aren't safe to print.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert i >= 0 and i < 256\n",
    "    return 'â–¡' if i not in PRINTABLE else str(chr(i))\n",
    "\n",
    "def t(blist):\n",
    "    return torch.tensor([int(b) for b in blist], dtype=torch.uint8)\n",
    "\n",
    "def gen_sentence(sent=SENT, g=TOY):\n",
    "\n",
    "    symb = '_[a-z]*'\n",
    "\n",
    "    while True:\n",
    "\n",
    "        match = re.search(symb, sent)\n",
    "        if match is None:\n",
    "            return sent\n",
    "\n",
    "        s = match.span()\n",
    "        sent = sent[:s[0]] + random.choice(g[sent[s[0]:s[1]]]) + sent[s[1]:]\n",
    "\n",
    "def load_toy(ntrain=100_000, ntest=20_000, to_torch=True, final=False, seed=0):\n",
    "    \"\"\"\n",
    "    Generates language from a toy grammar.\n",
    "    :param ntrain:\n",
    "    :param ntest:\n",
    "    :param to_torch: Whether to return torch tensors (if false, returns python lists)\n",
    "    :param final: Whether to return the test set or the validation set (True for test)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    train, test = '', ''\n",
    "    while len(train) < ntrain:\n",
    "        train += gen_sentence() + ' . '\n",
    "\n",
    "    random.seed(seed if final else seed + 1)\n",
    "    # -- change the seed so we get different test/val sets depending on `final`\n",
    "\n",
    "    while len(test) < ntest:\n",
    "        test += gen_sentence() + ' . '\n",
    "\n",
    "    ctr = Counter(train + test)\n",
    "    i2t = [PAD, START, END, UNK] + [t for t, _ in ctr.most_common()]\n",
    "    t2i = { w : i for  i, w in enumerate(i2t)}\n",
    "\n",
    "    train = [t2i[t] for t in train]\n",
    "    test  = [t2i[t] for t in test]\n",
    "    \n",
    "    if to_torch:\n",
    "        return (t(train), t(test)), (i2t, t2i) # Torch vectors (this takes a few seconds)\n",
    "\n",
    "    return (train, test), (i2t, t2i)\n",
    "\n",
    "def load_wp(fname='enwik8.gz', split=(90, 5, 5), to_torch=True, final=False):\n",
    "    \"\"\"\n",
    "    Load the enwik8 dataset from the Hutter challenge as a list or vector of bytes.\n",
    "    :param fname: Filename for the downloaded data.\n",
    "    :param split: Percentages for the train/val/test split.\n",
    "    :param to_torch: Whether to return torch tensors (True) or python lists (False)\n",
    "    :param final: If False, returns train/val if True returns train/test with the validation\n",
    "    data added to the training data.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(fname):\n",
    "        # If it doesn't exist, download it\n",
    "        print('Downloading')\n",
    "        wget.download(WP_DATA, out=fname)\n",
    "        \n",
    "    with gzip.open(fname, 'r') if fname.endswith('.gz') else open(fname, 'rb') as file:\n",
    "\n",
    "        all = file.read()\n",
    "        ctr = Counter(all)\n",
    "\n",
    "        i2t = {token : cas(token) for token, freq in ctr.most_common()}\n",
    "        t2i = {w : i for i, w in enumerate(i2t)}\n",
    "\n",
    "        split = tuple(s/sum(split) for s in split)\n",
    "        split = tuple(int(s * len(all)) for s in split)\n",
    "\n",
    "        train, val, test = all[:split[0]], all[split[0]:split[0]+split[1]], all[split[0]+split[1]:]\n",
    "\n",
    "        if final:\n",
    "            train = train + val\n",
    "            wh = test\n",
    "        else:\n",
    "            wh = val\n",
    "\n",
    "        if to_torch:\n",
    "            return (t(train), t(wh)), (i2t, t2i)\n",
    "\n",
    "        return (train, wh), (i2t, t2i)\n",
    "\n",
    "\n",
    "def load_xor(ntrain=25_000, ntest=25_000, seed=0):\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    i2w = [PAD, START, END, UNK, 'true', 'false'] #\n",
    "    w2i = {w : i for i, w in enumerate(i2w)}\n",
    "\n",
    "    dataset, labels = [], []\n",
    "    for _ in range(ntrain + ntest):\n",
    "        sentence = [\n",
    "            choice((i2w[4], i2w[5])),\n",
    "            choice((i2w[4], i2w[5]))\n",
    "        ]\n",
    "\n",
    "        f1, f2 = (sentence[0] == i2w[4]), (sentence[1] == i2w[4]) # true: very/great false: not/terrible\n",
    "        # -- these words are the only meaningful features\n",
    "        label = 0 if f1 != f2 else 1\n",
    "\n",
    "        dataset.append([w2i[word] for word in sentence])\n",
    "        labels.append(label)\n",
    "\n",
    "    return \\\n",
    "        (dataset[:ntrain], labels[:ntrain]), \\\n",
    "        (dataset[ntrain:], labels[ntrain:]), \\\n",
    "        (i2w, w2i), 2\n",
    "\n",
    "def load_imdb_synth(ntrain=25_000, ntest=25_000, seed=0):\n",
    "    \"\"\"\n",
    "    Synthetic IMDb dataset\n",
    "    :param seed:\n",
    "    :param voc:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    random.seed(seed)\n",
    "\n",
    "    adjectives = ['classic', 'silent', 'modern', 'vintage', 'independent', 'foreign', 'animated', 'documentary',\n",
    "    'epic', 'dramatic', 'romantic', 'comic', 'thrilling', 'mysterious', 'gritty', 'stylized', 'iconic', 'acclaimed',\n",
    "    'popular', 'forgettable', 'unreleased', 'awardwinning', 'blockbuster', 'lowbudget', 'highbudget', 'experimental',\n",
    "    'mainstream', 'cult', 'notable', 'original']\n",
    "    nouns = ['movie', 'film', 'motion-picture', 'feature', 'featurette', 'picture', 'flick', 'cinema', 'screenplay',\n",
    "    'blockbuster', 'talkie', 'silent', 'biopic', 'short', 'docudrama', 'documentary', 'animation', 'cartoon',\n",
    "    'anime', 'telefilm', 'miniseries', 'drama', 'comedy', 'thriller', 'western', 'musical', 'noir']\n",
    "    verbs = ['was', 'is', 'became', 'becomes', 'seemed', 'seems']\n",
    "\n",
    "    i2w = [PAD, START, END, UNK, 'this', 'not', 'very', 'great','terrible'] + verbs + adjectives + nouns\n",
    "    w2i = {w : i for i, w in enumerate(i2w)}\n",
    "\n",
    "    dataset, labels = [], []\n",
    "    for _ in range(ntrain + ntest):\n",
    "        sentence = [\n",
    "            i2w[4], # this\n",
    "            choice(adjectives), # old\n",
    "            choice(nouns), # movie\n",
    "            choice(verbs), # was\n",
    "            choice((i2w[5], i2w[6])),\n",
    "            choice((i2w[7], i2w[8]))\n",
    "        ]\n",
    "\n",
    "        f1, f2 = (sentence[4] == i2w[6]), (sentence[5] == i2w[7]) # true: very/great false: not/terrible\n",
    "        # -- these words are the only meaningful features\n",
    "        label = 0 if f1 != f2 else 1\n",
    "\n",
    "        dataset.append([w2i[word] for word in sentence])\n",
    "        labels.append(label)\n",
    "\n",
    "    return \\\n",
    "        (dataset[:ntrain], labels[:ntrain]), \\\n",
    "        (dataset[ntrain:], labels[ntrain:]), \\\n",
    "        (i2w, w2i), 2\n",
    "\n",
    "\n",
    "def load_imdb(final=False, val=5000, seed=0, voc=None, char=False):\n",
    "\n",
    "    cst = 'char' if char else 'word'\n",
    "\n",
    "    imdb_url = IMDB_URL.format(cst)\n",
    "    imdb_file = IMDB_FILE.format(cst)\n",
    "\n",
    "    if not os.path.exists(imdb_file):\n",
    "        wget.download(imdb_url)\n",
    "\n",
    "    with gzip.open(imdb_file) as file:\n",
    "        sequences, labels, i2w, w2i = pickle.load(file)\n",
    "\n",
    "    if voc is not None and voc < len(i2w):\n",
    "        nw_sequences = {}\n",
    "\n",
    "        i2w = i2w[:voc]\n",
    "        w2i = {w: i for i, w in enumerate(i2w)}\n",
    "\n",
    "        mx, unk = voc, w2i['.unk']\n",
    "        for key, seqs in sequences.items():\n",
    "            nw_sequences[key] = []\n",
    "            for seq in seqs:\n",
    "                seq = [s if s < mx else unk for s in seq]\n",
    "                nw_sequences[key].append(seq)\n",
    "\n",
    "        sequences = nw_sequences\n",
    "\n",
    "    if final:\n",
    "        return (sequences['train'], labels['train']), (sequences['test'], labels['test']), (i2w, w2i), 2\n",
    "\n",
    "    # Make a validation split\n",
    "    random.seed(seed)\n",
    "\n",
    "    x_train, y_train = [], []\n",
    "    x_val, y_val = [], []\n",
    "\n",
    "    val_ind = set( random.sample(range(len(sequences['train'])), k=val) )\n",
    "    for i, (s, l) in enumerate(zip(sequences['train'], labels['train'])):\n",
    "        if i in val_ind:\n",
    "            x_val.append(s)\n",
    "            y_val.append(l)\n",
    "        else:\n",
    "            x_train.append(s)\n",
    "            y_train.append(l)\n",
    "\n",
    "    return (x_train, y_train), \\\n",
    "           (x_val, y_val), \\\n",
    "           (i2w, w2i), 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c3f03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the IMDb dataset\n",
    "(x_train, y_train), (x_val, y_val), (i2w, w2i), numcls = load_imdb(final=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da873646",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m [\u001b[43mi2w\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m x_train[:\u001b[32m5\u001b[39m]]\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": [
    "[i2w[w] for w in x_train[:5]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d7e599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[14, 19, 9, 379, 22, 11, 50, 52, 53, 290],\n",
       "  [13, 574, 25, 809, 14, 32, 63, 26, 2722, 2231, 312],\n",
       "  [10721, 4, 10956, 129, 6, 124, 88114, 5, 6, 19, 93, 4118],\n",
       "  [198, 351, 17697, 116, 31, 13, 80, 40, 1240, 8, 69, 272, 883, 1749],\n",
       "  [60, 913, 366, 19, 118, 836, 44, 431, 902, 60, 286, 35, 34, 1834, 11]],\n",
       " [1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:5], y_train[:5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467e3da1",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "To-Do:\n",
    "1. Select a batch of the IMDB dataset.\n",
    "2. Determine the longest sequence in the batch.\n",
    "3. Pad all other sequences in the batch with the pad token index (w2i[\".pad\"]).\n",
    "4. Convert the result to a tensor of type torch.long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa390426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 30])\n",
      "Labels shape: torch.Size([32])\n",
      "Padding token index: 0\n",
      "\n",
      "First sequence in batch (first 20 tokens):\n",
      "tensor([ 14,  19,   9, 379,  22,  11,  50,  52,  53, 290,   0,   0,   0,   0,\n",
      "          0,   0,   0,   0,   0,   0])\n",
      "\n",
      "Words: ['this', 'movie', 'is', 'terrible', 'but', 'it', 'has', 'some', 'good', 'effects', '.pad', '.pad', '.pad', '.pad', '.pad', '.pad', '.pad', '.pad', '.pad', '.pad']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def prepare_batch(sequences, labels, batch_size, w2i):\n",
    "    \"\"\"\n",
    "    Prepare batches with padding for transformer input.\n",
    "    \n",
    "    Args:\n",
    "        sequences: List of sequences (each sequence is a list of token indices)\n",
    "        labels: List of labels corresponding to sequences\n",
    "        batch_size: Size of each batch\n",
    "        w2i: Word to index dictionary (contains the padding token)\n",
    "    \n",
    "    Yields:\n",
    "        Tuple of (padded_batch, labels_batch) where padded_batch is a tensor\n",
    "    \"\"\"\n",
    "    # Get the padding token index\n",
    "    pad_idx = w2i[PAD]\n",
    "    \n",
    "    # Process data in batches\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        # Get batch slice\n",
    "        batch_sequences = sequences[i:i + batch_size]\n",
    "        batch_labels = labels[i:i + batch_size]\n",
    "        \n",
    "        # Find max length in this batch\n",
    "        max_len = max(len(seq) for seq in batch_sequences)\n",
    "        \n",
    "        # Pad sequences to max length\n",
    "        padded_batch = []\n",
    "        for seq in batch_sequences:\n",
    "            # Pad sequence to max_len\n",
    "            padded_seq = seq + [pad_idx] * (max_len - len(seq))\n",
    "            padded_batch.append(padded_seq)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        batch_tensor = torch.tensor(padded_batch, dtype=torch.long)\n",
    "        labels_tensor = torch.tensor(batch_labels, dtype=torch.long)\n",
    "        \n",
    "        yield batch_tensor, labels_tensor\n",
    "\n",
    "\n",
    "# Example usage: create a batch\n",
    "batch_size = 32\n",
    "batches = list(prepare_batch(x_train[:100], y_train[:100], batch_size, w2i))\n",
    "\n",
    "# Show first batch\n",
    "first_batch, first_labels = batches[0]\n",
    "print(f\"Batch shape: {first_batch.shape}\")\n",
    "print(f\"Labels shape: {first_labels.shape}\")\n",
    "print(f\"Padding token index: {w2i[PAD]}\")\n",
    "print(f\"\\nFirst sequence in batch (first 20 tokens):\")\n",
    "print(first_batch[0][:20])\n",
    "print(f\"\\nWords: {[i2w[idx.item()] for idx in first_batch[0][:20]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc1efc",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de02230f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple classification model with embedding layer and global pooling.\n",
    "\n",
    "    Architecture:\n",
    "    1. Embedding layer: converts token indices to embedding vectors\n",
    "    2. Global pooling: pools along the time dimension\n",
    "    3. Linear layer: projects to number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, emb_size, num_classes, pooling=\"max\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: Number of tokens in vocabulary\n",
    "            emb_size: Size of embedding vectors (300)\n",
    "            num_classes: Number of output classes\n",
    "            pooling: Type of pooling ('max', 'mean', or 'sum')\n",
    "        \"\"\"\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_size)\n",
    "        self.linear = nn.Linear(emb_size, num_classes)\n",
    "        self.pooling = pooling\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch, time) with dtype=torch.long\n",
    "\n",
    "        Returns:\n",
    "            Output tensor of shape (batch, num_classes) with dtype=torch.float\n",
    "        \"\"\"\n",
    "        # Step 1: Embedding layer\n",
    "        # Input: (batch, time) -> Output: (batch, time, emb)\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # Step 2: Global pooling along time dimension (dim=1)\n",
    "        # Input: (batch, time, emb) -> Output: (batch, emb)\n",
    "        if self.pooling == \"max\":\n",
    "            pooled, _ = torch.max(embedded, dim=1)  # max pooling\n",
    "        elif self.pooling == \"mean\":\n",
    "            pooled = torch.mean(embedded, dim=1)  # mean pooling\n",
    "        elif self.pooling == \"sum\":\n",
    "            pooled = torch.sum(embedded, dim=1)  # sum pooling\n",
    "        \n",
    "        elif self.pooling == \"select\":\n",
    "            # Example: select the first time step\n",
    "            pooled = embedded[:, 0, :]  # select pooling\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown pooling type: {self.pooling}\")\n",
    "\n",
    "        # Step 3: Linear projection to number of classes\n",
    "        # Input: (batch, emb) -> Output: (batch, num_classes)\n",
    "        output = self.linear(pooled)\n",
    "\n",
    "        # Note: We don't apply softmax here - it's included in cross_entropy loss\n",
    "        return output\n",
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "emb_size = 300\n",
    "vocab_size = len(i2w)  # Get vocabulary size from i2w\n",
    "num_classes = numcls  # Number of classes (2 for binary classification)\n",
    "\n",
    "# Create model\n",
    "model = SimpleClassifier(vocab_size, emb_size, num_classes, pooling=\"mean\")\n",
    "\n",
    "print(f\"Model created:\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")\n",
    "print(f\"  Embedding size: {emb_size}\")\n",
    "print(f\"  Number of classes: {num_classes}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)\n",
    "\n",
    "# Test with a small batch\n",
    "test_batch, test_labels = next(\n",
    "    prepare_batch(x_train, y_train, batch_size=500, w2i=w2i)\n",
    ")\n",
    "print(f\"\\nTest batch shape: {test_batch.shape}\")\n",
    "output = model(test_batch)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output dtype: {output.dtype}\")\n",
    "\n",
    "# Calculate loss using cross_entropy (includes softmax)\n",
    "loss = F.cross_entropy(output, test_labels)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")\n",
    "\n",
    "# Calculate accuracy\n",
    "predictions = torch.argmax(output, dim=1)\n",
    "accuracy = (predictions == test_labels).float().mean()\n",
    "print(f\"Accuracy: {accuracy.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
